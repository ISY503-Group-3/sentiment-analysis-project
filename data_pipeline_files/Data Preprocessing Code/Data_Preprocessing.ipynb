{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Engineering Override: ETL Pipeline & Ground Truth Restoration\n",
        "**Objective:** Restore data integrity by re-engineering the ETL (Extract, Transform, Load) pipeline from the raw source.\n",
        "\n",
        "**Context & Justification:**\n",
        "The initial datasets provided contained synthetic labels generated by an untrained model. To satisfy the requirement for **Supervised Learning**, we must train on **Ground Truth** (human-verified) labels. This section implements a custom parser to extract the original sentiment labels from the raw Johns Hopkins University (JHU) XML corpus.\n",
        "\n",
        "**Technical Implementation:**\n",
        "1.  **Ingestion:** We programmatically retrieve the `domain_sentiment_data.tar.gz` directly from the academic source to ensure reproducibility.\n",
        "2.  **XML Parsing:** The raw data is unstructured pseudo-XML. We utilize `BeautifulSoup` to traverse the DOM tree and extract the text content within `<review_text>` tags.\n",
        "3.  **Label Encoding:**\n",
        "> Files located in `positive.review` are explicitly mapped to **Label 1**. <br>Files located in `negative.review` are explicitly mapped to **Label 0**. <br>This guarantees a deterministic and correct binary classification target, resolving the label noise issues found in the previous iteration.\n",
        "4.  **Persistence:** The cleaned data is aggregated into structured DataFrames and serialized as CSVs (e.g., `books_clean.csv`, `dvd_clean.csv`, etc.), serving as the immutable input for the Model Architecture phase."
      ],
      "metadata": {
        "id": "hxYgatKMt5Q6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfyxLye4g4h5"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ================================================================ #\n",
        "# ISY503: Intelligent Systems - Final Project (Data Preprocessing) #\n",
        "# ================================================================ #\n",
        "\n",
        "# ------------------- #\n",
        "# 1. Data Engineering #\n",
        "# ------------------- #\n",
        "# This section downloads the raw data and creates the \"Ground Truth\" files.\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# 1. Download the Original JHU Dataset\n",
        "url = \"https://www.cs.jhu.edu/~mdredze/datasets/sentiment/domain_sentiment_data.tar.gz\"\n",
        "filename = \"domain_sentiment_data.tar.gz\"\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    print(\"Downloading raw dataset... (approx 50MB)\")\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "# 2. Extract Data\n",
        "if not os.path.exists(\"sorted_data_acl\"):\n",
        "    print(\"Extracting files...\")\n",
        "    with tarfile.open(filename, \"r:gz\") as tar:\n",
        "        tar.extractall()\n",
        "    print(\"Extraction complete.\")\n",
        "\n",
        "# 3. Parse & Create Clean CSVs (The Logic Member 1 missed)\n",
        "def parse_review_file(file_path, label_value):\n",
        "    \"\"\"Parses pseudo-XML to extract review text.\"\"\"\n",
        "    texts = []\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # The files are pseudo-XML. BeautifulSoup handles them well.\n",
        "    soup = BeautifulSoup(content, \"html.parser\")\n",
        "    reviews = soup.find_all(\"review_text\")\n",
        "\n",
        "    for r in reviews:\n",
        "        texts.append(r.get_text().strip())\n",
        "    return texts\n",
        "\n",
        "categories = ['books', 'dvd', 'electronics', 'kitchen_&_housewares']\n",
        "dfs = []\n",
        "\n",
        "print(\"\\nProcessing raw files into Clean CSVs...\")\n",
        "for cat in categories:\n",
        "    # Path to positive and negative files\n",
        "    pos_path = os.path.join(\"sorted_data_acl\", cat, \"positive.review\")\n",
        "    neg_path = os.path.join(\"sorted_data_acl\", cat, \"negative.review\")\n",
        "\n",
        "    # Parse\n",
        "    pos_reviews = parse_review_file(pos_path, 1)\n",
        "    neg_reviews = parse_review_file(neg_path, 0)\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_pos = pd.DataFrame({'review': pos_reviews, 'label': 1, 'category': cat})\n",
        "    df_neg = pd.DataFrame({'review': neg_reviews, 'label': 0, 'category': cat})\n",
        "\n",
        "    # Combine & Save\n",
        "    df_cat = pd.concat([df_pos, df_neg], ignore_index=True)\n",
        "    csv_name = f\"{cat}_clean.csv\"\n",
        "    df_cat.to_csv(csv_name, index=False)\n",
        "\n",
        "    dfs.append(df_cat)\n",
        "    print(f\"âœ… Created {csv_name} ({len(df_cat)} rows: {len(df_pos)} pos, {len(df_neg)} neg)\")\n",
        "\n",
        "# Combine all for training\n",
        "full_data = pd.concat(dfs, ignore_index=True)\n",
        "print(f\"\\nTotal Dataset Size: {len(full_data)} rows\")"
      ]
    }
  ]
}